{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sleep_Analysis_Challenge_basicCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB6YunD0Gdgc",
        "colab_type": "text"
      },
      "source": [
        "# \"You Snooze, You Win\" Challenge\n",
        "\n",
        "Every year, the [PhysioNet/CinC (Computing in Cardiology) Challenge](https://www.physionet.org/challenge/) invites \"participants to tackle clinically interesting problems that are either unsolved or not well-solved.\" For this year's week 2 machine learning challenge, BWSI has revived a past PhysioNet challenge based on sleep classification.\n",
        "\n",
        "This year's challenge focuses on the classification of nonarousal and arousal timeframes. If you would like to understand the biological implications of the challenge, we recommend reading PhysioNet's [introduction](https://physionet.org/challenge/2018/) of the challenge.\n",
        "\n",
        "For this challenge, you will classify samples into 5 classes (Arousal, NREM1, NREM2, NREM3, REM). Each sample consists of seven physiological signals (O2-M1, E1-M2, Chin1-Chin2, ABD, CHEST, AIRFLOW, ECG) measured at 200 Hz over a 60 second period (12000 timepoints). In this notebook, we provide code to import the data, visualize sample signals, implement an example classifier, and 'score' your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ctSzKHoGr1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Import libraries ###\n",
        "\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "#set default plotting fonts\n",
        "font = {'family' : 'sans-serif',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 20}\n",
        "\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "\n",
        "#data preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#garbage collection (for saving RAM during training)\n",
        "import gc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XxIE6ZwGvjh",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "This dataset is a modified version of the PhysioNet/CinC Challenge data, which were contributed by the Massachusetts General Hospital’s Computational Clinical Neurophysiology Laboratory, and the Clinical Data Animation Laboratory.\n",
        "***\n",
        "**Class labels:**\n",
        "- 0 = Arousal\n",
        "- 1 = NREM1\n",
        "- 2 = NREM2\n",
        "- 3 = NREM3\n",
        "- 4 = REM\n",
        "***\n",
        "**Class descriptions:**\n",
        "\n",
        "<img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/blob/master/Images/Week2/sleepStagesTable.svg?raw=true\">\n",
        "\n",
        "***\n",
        "**Physiological signal description:**\n",
        "\n",
        "O2-M1 - posterior brain activity (electroencephalography)\n",
        "\n",
        "E1-M2 - left eye activity (electrooculography)\n",
        "\n",
        "Chin1-Chin2 - chin movement (electromyography)\n",
        "\n",
        "ABD - abdominal movement (electromyography)\n",
        "\n",
        "CHEST - chest movement (electromyography)\n",
        "\n",
        "AIRFLOW - respiratory airflow\n",
        "\n",
        "ECG - cardiac activity (electrocardiography)\n",
        "***\n",
        "Run both cell blocks to get the challenge data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfgbZPNYziXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone repo and move into data directory (only run this once)\n",
        "!git clone https://github.com/BeaverWorksMedlytics2020/Data_Public\n",
        "os.chdir('./Data_Public/ChallengeProjects/Week2/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qANF2BvQG2m3",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data in Memory\n",
        "Run the cell below to extract the raw training and test data. It may take a minute or two to run through. Here are the variables containing the data you will get:\n",
        "\n",
        "* **data_train**: np array shape (4000, 12000, 7). Contains 4000 samples (60s each) of 12000 data points (200Hz x 60s), for 7 different signals. \n",
        "* **labels_train**: np array shape (4000,). Contains ground truth labels for data_train. The order of the labels corresponds to the order of the training data.\n",
        "* **ID_train**: list of 4000 unique IDs. The order of the IDs corresponds to the order of the training data. \n",
        "* **data_test**: np array shape (1000, 12000, 7). Contains 1000 samples (60s each) of 12000 data points (200Hz x 60s), for 7 different signals.\n",
        "* **ID_test**: list of 1000 unique IDs. The order of the IDs corresponds to the order of the training data.\n",
        "\n",
        "We encourage you to print each of these variables to see what they look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rw8inOvG5QP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run once to import data ###\n",
        "\n",
        "def get_file_locs():\n",
        "    '''\n",
        "    find all files in 'training' and 'test' directories and put their names \n",
        "    under 'training' and 'test' keys in the file_dict dictionary\n",
        "    '''\n",
        "\n",
        "    file_dict = {'training':[], 'test':[]}\n",
        "    for data_type in file_dict:\n",
        "        for file in os.listdir('./' + data_type):\n",
        "            file_dict[data_type].append(data_type + '/' + file)\n",
        "    \n",
        "    return file_dict\n",
        "\n",
        "def get_sample_data(data_type, id_number):\n",
        "    '''\n",
        "    get signal data, label, and filename associated with given data type and index num\n",
        "\n",
        "    parameters:\n",
        "\n",
        "     data_type -- Dictates whether sample comes from training set or test set.\n",
        "                 This input must be either 'training' or 'test' (defaults to 'training')\n",
        "\n",
        "     id_number -- Which sample ID should be returned? Must be 0-3999 if data_type is 'training'\n",
        "                 or 0-999 if data_type is 'test' (defaults to random integer from 0-999)\n",
        "  \n",
        "    returns:\n",
        "\n",
        "     sample_data -- dataframe with 1 row and 2 columns-- column \"Signal\" contains a series object \n",
        "                    and column \"Label\" contains numeric label for that sample\n",
        "    '''\n",
        "    file = './' + data_type + '/' + str(id_number) + '.xz'\n",
        "\n",
        "    #sample_data is a dataframe with 1 row and 2 columns--\n",
        "    #\"Signal\" (contains a series object) and \"Label\" (contains numeric label)\n",
        "    sample_data = pd.read_pickle('./' + file)\n",
        "\n",
        "    return sample_data, file.split('/')[2]\n",
        "\n",
        "file_dict = get_file_locs()\n",
        "print(f\"{len(file_dict['training'])} training samples found, {len(file_dict['test'])} test samples found\")\n",
        "\n",
        "data_train = np.zeros((4000, 12000, 7))\n",
        "labels_train = np.zeros(4000)\n",
        "ID_train = []\n",
        "for i in range(4000):\n",
        "  sample_data, ID = get_sample_data('training', i)\n",
        "  data_train[i] = np.array(list(sample_data['Signal']), dtype=np.float).reshape(12000, 7)\n",
        "  labels_train[i] = np.array(list(sample_data['Label']), dtype=np.float)\n",
        "  ID_train.append(ID)\n",
        "  if(i%500==0):\n",
        "    print('Loading training sample ' + str(i))\n",
        "  \n",
        "data_test = np.zeros((1000, 12000, 7))\n",
        "ID_test = []\n",
        "for i in range(1000):\n",
        "  sample_data, ID = get_sample_data('test', i)\n",
        "  data_test[i] = np.array(list(sample_data['Signal']), dtype=np.float).reshape(12000, 7)\n",
        "  ID_test.append(ID)\n",
        "  if(i%500==0):\n",
        "    print('Loading test sample ' + str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVNgpgciw6p4",
        "colab_type": "text"
      },
      "source": [
        "## Create One-Hot Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN0BBD3fHKRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Create label array for all training samples using categorical datatype ###\n",
        "\n",
        "train_labels = np.ndarray(shape = (1, 4000))\n",
        "\n",
        "#set labels to integers first\n",
        "for i in range(4000):\n",
        "    train_labels[0][i] = i//800 # This is a way to label each entry (since classes are in order)\n",
        "\n",
        "#convert labels to onehot, ensure type is float32\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels[0], 5).astype(np.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIzqcXmJBHdX",
        "colab_type": "text"
      },
      "source": [
        "## Shuffle and Partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "buuR3HH9yTOw",
        "colab": {}
      },
      "source": [
        "### Shuffle and partition all train data\n",
        "\n",
        "#(Training data is ordered by default so shuffling before partitioning is important)\n",
        "\n",
        "#--Shuffle data_train--\n",
        "#(Note that data is only shuffled in first dimension, which is what we want)\n",
        "data_train, train_labels = shuffle(data_train, train_labels, random_state = 25, stratify = train_labels)\n",
        "\n",
        "#--Scale all labeled data in data_train--\n",
        "\n",
        "#initialize standard scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#Standard scaler is meant for 2D arrays, so we reshape, scale, and then reshape again\n",
        "reshaped_X_train = data_train.reshape((data_train.shape[0]*data_train.shape[1], data_train.shape[2])).copy()\n",
        "reshaped_X_train = scaler.fit_transform(reshaped_X_train)\n",
        "data_train = reshaped_X_train.reshape((data_train.shape[0], data_train.shape[1], data_train.shape[2]))\n",
        "\n",
        "del reshaped_X_train #get rid of large temporary variable\n",
        "\n",
        "#--Scale unlabeled test data--\n",
        "#Because we scaled labeled data before training, we need to also scale test data --\n",
        "\n",
        "#Standard scaler is meant for 2D arrays, so we reshape, apply scaling, and then \n",
        "#reshape again to get back to original\n",
        "reshaped_X_test = data_test.reshape((data_test.shape[0]*data_test.shape[1], data_test.shape[2])).copy()\n",
        "reshaped_X_test = scaler.transform(reshaped_X_test)\n",
        "data_test = reshaped_X_test.reshape((data_test.shape[0], data_test.shape[1], data_test.shape[2]))\n",
        "\n",
        "del reshaped_X_test #get rid of large temporary variable\n",
        "\n",
        "#--create 3 partitions of provided training data--\n",
        "# Note we are breaking up provided labeled data into training, validation, and \"mock test\" sets\n",
        "\n",
        "val_size = 1000\n",
        "mocktest_size = 500\n",
        "\n",
        "mocktest_data = data_train[0:mocktest_size, :, :]\n",
        "mocktest_labels = train_labels[0:mocktest_size, :]\n",
        "\n",
        "val_data = data_train[mocktest_size:mocktest_size+val_size, :, :]\n",
        "val_labels = train_labels[mocktest_size:mocktest_size+val_size, :]\n",
        "\n",
        "partial_train_data = data_train[mocktest_size+val_size:,:,:]\n",
        "tr_labels = train_labels[mocktest_size+val_size:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOcj_uuUHP3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run every time you change set of parameters ###\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "\"\"\" Modify to your heart's and algorithm's content ^_^ \"\"\"\n",
        "\n",
        "model.add(tf.keras.layers.Conv1D(filters = 10, kernel_size = 50, padding = 'valid',\n",
        "                                 activation=tf.nn.relu, \n",
        "                                 input_shape=(partial_train_data.shape[1],partial_train_data.shape[2])))\n",
        "\n",
        "#take maximum activation value from each convolution result and pass to next layer\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "#insert a dense layer with 64 units\n",
        "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "\n",
        "# we should end with a softmax to ensure outputs behave like probabilities\n",
        "#(i.e. sum to 1)\n",
        "model.add(tf.keras.layers.Dense(5, activation=tf.nn.softmax)) \n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "#Another potential optimizer\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhhsAXXJHRvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run whenever you want to train and validate your model ###\n",
        "\n",
        "\"\"\"\n",
        "EPOCHS (int) the number of times the optimization algorithm passes\n",
        "through the full dataset (calculating errors and derivatives) to update weights\n",
        "(One pass through the data is called an \"epoch\")\n",
        "\"\"\"\n",
        "\n",
        "#This function is called after each epoch\n",
        "#(It will ensure that your training process does not consume all available RAM)\n",
        "class garbage_collect_callback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    gc.collect()\n",
        "\n",
        "history = model.fit(partial_train_data, # Train examples\n",
        "          tr_labels, # Train labels\n",
        "          epochs=10, # number of epochs (passes through data during training)\n",
        "          batch_size=600, # number of points to consider in each optimizer iteration\n",
        "          callbacks = [garbage_collect_callback()],\n",
        "          validation_data=(val_data, val_labels), #data to use for validation\n",
        "          verbose=1) #will print information about optimization process\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRk6Xh_LHTXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run once after you have finished training your model ###\n",
        "#Reminder: Please ensure order of test data points has not been changed.\n",
        "\n",
        "#-- Evaluate model for test data --\n",
        "test_pred = model.predict(data_test)\n",
        "test_output = np.ndarray(shape = (1000, 6))\n",
        "\n",
        "#-- Write test output to dataframe and save to pickle file --\n",
        "test_dataframe = pd.DataFrame(test_output)\n",
        "\n",
        "file = 'unladen_swallow.xz'\n",
        "test_dataframe.to_pickle(file)\n",
        "test_dataframe = pd.DataFrame(test_output)\n",
        "os.listdir('.')\n",
        "files.download(file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bduvOuTtHVfK",
        "colab_type": "text"
      },
      "source": [
        "## Submitting Your Model\n",
        "\n",
        "After training your classifier, run it on the test data to generate your predictions. Each class for a test sample should have an associated probability (between 0 and 1). Below are the parameters for the prediction format and export:\n",
        "\n",
        "- Your predictions should be in a pandas dataframe with 5 columns (classes) and 1000 rows (samples). Note that your predictions must follow the original test sample order (0.xz, 1.xz, 2.xz, ...). You only need to worry about this if you shuffled the test samples or stored the samples in an unordered data structure (dictionaries and sets). If this is the case, you should 1) add a separate column in your pandas dataframe with the file number for each sample; 2) sort the dataframe using this column; and 3) drop the column. These steps have been noted in the code below.\n",
        "- The predictions dataframe should be exported as an .xz file using dataframe.to_pickle() followed by files.download().\n",
        "\n",
        "Example code of the prediction format and export is presented in the cell block below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TiEUxHcZz_D",
        "colab_type": "text"
      },
      "source": [
        "Your model will be evaluated on Area Under the ROC Curve (ROCAUC), Matthews Correlation Coefficient (MCC) and creativity. There will be a \"winning\" group for each of these categories.\n",
        "\n",
        "If you are finished early, consider trying other ML algorithms and/or implementing multiple feature extraction methods. You can also help other groups if you finish early."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDQWbhUK6cMt",
        "colab_type": "text"
      },
      "source": [
        "## How Your Model Will Be Evaluated\n",
        "\n",
        "- **Area Under the ROC Curve (AUCROC)**: The receiver operating characteristic (ROC) curve plots the true positive rate (sensitivity/recall) against the false positive rate (fall-out) at many decision threshold settings. The area under the curve (AUC) measures discrimination, the classifier's ability to correctly identify samples from the \"positive\" and \"negative\" cases. Intuitively, AUC is the probability that a randomly chosen \"positive\" sample will be labeled as \"more positive\" than a randomly chosen \"negative\" sample. In the case of a multi-class ROC curve, each class is considered separately before taking the weighted average of all the class results. Simply put, the class under consideration is labeled as \"positive\" while all other classes are labeled as \"negative.\" Below is the multi-class ROC curve for the example classifier. The AUCROC score should be between 0 and 1, in which 0.5 is random classification and 1 is perfect classification.\n",
        "\n",
        "<img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/blob/master/Images/Week2/MultiClassRocCurve_exampleClassifier.png?raw=true\" width=\"600\" height=\"500\">\n",
        "\n",
        "- **Matthews Correlation Coefficient (MCC)**: The MCC measures the quality of binary classifications, irrespective of the class sizes. Importantly, it is typically regarded as a balanced measure since it considers all values in the 2x2 contingency table (TP, FP, TN, FN). For this challenge, the binary classes will be \"Arousal\" (Arousal) and \"Nonarousal\" (NREM1, NREM2, NREM3, REM). The MCC score should be between -1 and 1, in which 0 is random classification and 1 is perfect classification.\n",
        "\n",
        " ![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/5caa90fc15105b74b59a30bbc9cc2e5bd43a13b7)\n",
        "\n",
        "Using these metrics, the example classifier has the following scores on test data:\n",
        "- AUCROC: 0.727\n",
        "- MCC: 0.163\n",
        "- Creativity: ( ͡° ͜ʖ ͡°)\n",
        "\n",
        "Below is the code used to calculate the AUCROC and MCC metrics when evaluating your classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOpioHig8688",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = pd.DataFrame(model.predict(mocktest_data))\n",
        "test_predict = test_pred.idxmax(axis=1)\n",
        "test_labels = [ np.where(label==1)[0][0] for label in mocktest_labels]\n",
        "test_labels_one_hot = pd.DataFrame(mocktest_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIlfwfksLp-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr = {}\n",
        "tpr = {}\n",
        "roc_auc = {}\n",
        "\n",
        "\"\"\" Initalize key reference dictionaries \"\"\"\n",
        "sig_dict = {0:'O2-M1', 1:'E1-M2', 2:'Chin1-Chin2', 3:'ABD', 4:'CHEST', 5:'AIRFLOW', 6:'ECG'}\n",
        "sig_type_dict = {0:'Time (s)', 1:'Frequency (Hz)'}\n",
        "stage_dict = {0:'Arousal', 1:'NREM1', 2:'NREM2', 3:'NREM3', 4:'REM'}\n",
        "\n",
        "plt.figure(figsize=(14,10))\n",
        "for i in range(5):\n",
        "    fpr[i], tpr[i], _ = metrics.roc_curve(test_labels_one_hot.iloc[:, i], test_pred.iloc[:, i])\n",
        "    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
        "    plt.plot(fpr[i], tpr[i], label = stage_dict[i] + ', ' + str(i))\n",
        "\n",
        "plt.plot([0, 1], [0, 1])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multi-Class ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(test_labels_one_hot.values.ravel(), test_pred.values.ravel())\n",
        "roc_auc_agg = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO1I5lme8oya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "for i in range(test_pred.shape[0]):\n",
        "    if test_predict.iloc[i]==0: y_pred.append(1)\n",
        "    else: y_pred.append(-1)\n",
        "    if test_labels[i]==0: y_true.append(1)\n",
        "    else: y_true.append(-1)\n",
        "mcc = metrics.matthews_corrcoef(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo7edcTTC6yD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(roc_auc_agg, mcc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}